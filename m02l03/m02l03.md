## Занятие 3: Введение в BeautifulSoup

### 1. Что такое BeautifulSoup и его установка в проект
---
#### Что такое BeautifulSoup

- **Описание:**
  - BeautifulSoup — это библиотека Python, предназначенная для парсинга и обработки HTML и XML документов.
  - Она упрощает извлечение данных из веб-страниц и их обработку.

- **Основные возможности:**
  - Парсинг HTML и XML документов.
  - Поиск и извлечение элементов по тегам, классам и атрибутам.
  - Модификация и обновление содержимого документов.
  - Поддержка различных парсеров, таких как `html.parser`, `lxml`, `xml` и `html5lib`.

#### Установка BeautifulSoup в проект

- **Шаг 1: Установка библиотеки BeautifulSoup:**
  - Установите BeautifulSoup (https://pypi.org/project/beautifulsoup4/) с помощью pip:
    ```bash
    pip install beautifulsoup4
    ```

- **Шаг 2: Установка парсера (опционально):**
  - BeautifulSoup поддерживает несколько парсеров. Рекомендуется установить один из них для повышения производительности.
  - Установка `lxml` (быстрый парсер на основе C: https://pypi.org/project/lxml/):
    ```bash
    pip install lxml
    ```


### 2. Что такое requests и его установка в проект
---

#### Что такое requests

- **Описание:**
  - Requests — это популярная библиотека Python для отправки HTTP-запросов.
  - Она упрощает работу с веб-API и получение данных из веб-страниц.

- **Основные возможности:**
  - Отправка различных типов HTTP-запросов (GET, POST, PUT, DELETE и т.д.).
  - Управление заголовками запросов и параметрами.
  - Обработка ответов, включая статус-коды и содержимое ответа.
  - Поддержка сессий для сохранения состояния между запросами.

- **Примеры использования:**
  - Получение данных с веб-страницы.
  - Взаимодействие с RESTful API.
  - Отправка данных на сервер через POST-запросы.

#### Установка requests в проект

- **Установка библиотеки Requests:**
  - Установите Requests (https://pypi.org/project/requests/) с помощью pip:
    ```bash
    pip install requests
    ```

- **Пример использования:**
  - Простой GET-запрос для получения своего IP:
    ```python
    import requests

    response = requests.get('https://ifconfig.me')
    print(response.status_code)  # Статус-код ответа
    print(response.text)         # Содержимое ответа
    ```

### 3. Основы работы с BeautifulSoup
---

#### Импорт библиотек

- **Импорт BeautifulSoup:**
  - Для работы с BeautifulSoup необходимо импортировать её из библиотеки `bs4`:
    ```python
    from bs4 import BeautifulSoup
    ```

- **Импорт дополнительной библиотеки `requests`:**
  - `requests` используется для получения HTML-кода веб-страницы:
    ```python
    import requests
    ```

- **Импорт парсера (опционально):**
  - Если установлен `lxml` или `html5lib`, вы можете использовать их для создания объекта BeautifulSoup:
    ```python
    import lxml
    import html5lib
    ```

#### Создание объекта BeautifulSoup

- **Создание объекта из строки HTML:**
  - Если у вас есть строка HTML, вы можете создать объект BeautifulSoup следующим образом:
    ```python
    html_doc = "<html><head><title>Title</title></head><body><p>Paragraph</p>bodytext</body></html>"
    soup = BeautifulSoup(html_doc, 'lxml')
    ```

- **Создание объекта из загруженного веб-документа:**
  - Используйте библиотеку `requests` для получения HTML-кода веб-страницы и затем создайте объект BeautifulSoup:
    ```python
    response = requests.get('https://example.com')
    soup = BeautifulSoup(response.text, 'lxml')
    ```

### 4. Основные методы и свойства
---

#### Получение содержимого тегов

- **Получение тега:**
  - Чтобы получить определённый тег из документа:
    ```python
    title_tag = soup.title
    print(title_tag)  # <title>Title</title>
    ```

- **Получение текстового содержимого тега:**
  - Для извлечения текста внутри тега используйте свойство `.text`:
    ```python
    title_text = soup.title.text
    print(title_text)  # Title
    ```

- **Получение текстового содержимого всего документа:**
  - Чтобы получить весь текст без HTML-тегов:
    ```python
    full_text = soup.get_text()
    print(full_text)  # TitleParagraph
    ```

#### Поиск элементов

- **Поиск по тегу:**
  - **Один элемент:**
    ```python
    p_tag = soup.find('p')
    print(p_tag)  # <p>Paragraph</p>
    ```

  - **Все элементы с данным тегом:**
    ```python
    p_tags = soup.find_all('p')
    for p in p_tags:
        print(p.get_text())  # Paragraph
    ```

- **Поиск по классу или идентификатору:**
  - **Поиск по CSS-классу:**
    ```python
    element_by_class = soup.find(class_='class-name')
    ```

  - **Поиск по идентификатору (ID):**
    ```python
    element_by_id = soup.find(id='id-name')
    ```

- **Поиск по атрибутам:**
  - **Поиск элементов с определённым атрибутом:**
    ```python
    link = soup.find('a', href=True)
    print(link['href'])  # Значение атрибута href
    ```

  - **Поиск элементов с определённым значением атрибута:**
    ```python
    specific_element = soup.find('a', href='https://example.com')
    ```

- **Использование CSS-селекторов:**
  - Можно использовать метод `select` для поиска элементов по CSS-селекторам:
    ```python
    elements = soup.select('div.class-name #id-name')
    for element in elements:
        print(element.get_text())
    ```

- **Использование XPath:**
  - В библиотеке BeautifulSoup нет встроенной поддержки XPath, поскольку BeautifulSoup использует свои собственные методы и синтаксис для поиска элементов в HTML и XML-документах. Однако, вы можете использовать XPath в комбинации с BeautifulSoup при помощи дополнительных инструментов и библиотек. Вот как это можно сделать при помощи `lxml`:
  - **Использование `lxml` для парсинга и выполнения XPath-запросов:**
    ```python
    from lxml import html
    import requests

    # Получение HTML-кода страницы
    response = requests.get('https://example.com')
    tree = html.fromstring(response.content)

    # Выполнение XPath-запроса
    titles = tree.xpath('//title/text()')
    print(titles)  # ['Title']
    ```

### 5. Работа с тегами
---

#### Получение контента тега

- **Получение тега:**
  - Чтобы получить конкретный тег из документа, используйте метод `find` для поиска первого вхождения тега или `find_all` для поиска всех вхождений.
    ```python
    # Получение первого тега <p>
    p_tag = soup.find('p')
    ```

- **Получение текста внутри тега:**
  - Чтобы извлечь текстовое содержимое тега, используйте свойство `.text` или метод `.get_text()`.
    ```python
    # Получение текста внутри первого тега <p>
    paragraph_text = p_tag.text
    print(paragraph_text) # Paragraphbodytext

    # Альтернативный способ получения текста
    full_text = p_tag.get_text()
    print(full_text) # Paragraphbodytext

    # Получение дочерних элементов в list объекте
    full_text = p_tag.contents
    print(full_text)  # [<p>Paragraph</p>, 'bodytext']
    ```

- **Получение текста из всех вхождений тега:**
  - Используйте `find_all` для получения списка всех элементов и извлечения текста из каждого:
    ```python
    # Получение всех тегов <p>
    p_tags = soup.find_all('p')
    for p in p_tags:
        print(p.get_text())
    ```

- **Получение контента тегов с определённым классом или идентификатором:**
  ```python
  # Поиск по классу
  class_elements = soup.find_all(class_='class-name')
  for element in class_elements:
      print(element.get_text())

  # Поиск по идентификатору
  id_element = soup.find(id='id-name')
  print(id_element.get_text())
  ```

#### Получение атрибутов тега

- **Получение значения атрибута:**
  - Для получения значения атрибута тега используйте синтаксис индексирования. Это применяется к атрибутам, таким как `href`, `src`, `class` и т.д.
    ```python
    # Получение первого тега <a>
    link_tag = soup.find('a')

    # Получение значения атрибута href
    href_value = link_tag['href']
    print(href_value)  # Например: "https://example.com"
    ```

- **Получение всех атрибутов тега:**
  - Для получения всех атрибутов тега используйте метод `.attrs`, который возвращает словарь всех атрибутов и их значений.
    ```python
    # Получение всех атрибутов тега
    attributes = link_tag.attrs
    print(attributes)  # Например: {'href': 'https://example.com', 'class': 'link-class'}
    ```

- **Проверка существования атрибута:**
  - Для проверки, существует ли атрибут, можно использовать метод `.get()`:
    ```python
    # Проверка наличия атрибута и получение его значения
    href_value = link_tag.get('href')
    print(href_value)  # Выводит значение href, если он существует
    ```

- **Изменение значения атрибута:**
  - Вы можете изменить значение атрибута, просто присвоив новое значение:
    ```python
    # Изменение значения атрибута href
    link_tag['href'] = 'https://new-url.com'
    ```

- **Добавление нового атрибута:**
  - Вы также можете добавить новый атрибут:
    ```python
    # Добавление нового атрибута
    link_tag['new-attribute'] = 'value'
    ```

- **Удаление атрибута:**
  - Для удаления атрибута используйте метод `del`:
    ```python
    # Удаление атрибута
    del link_tag['href']
    ```

### Домашнее задание
При помощи CSS selector, которые вы создали на прошлом занятии, вытащите содержимое с таргет страницы: заголовок таблицы, содержимое строк данных и email