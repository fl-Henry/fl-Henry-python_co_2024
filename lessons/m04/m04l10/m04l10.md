# Занятие 10: Оптимизация работы с Pandas

## 1. Введение в оптимизацию в Pandas

### Проблемы производительности при работе с большими данными

Работа с большими данными (например, с датафреймами, содержащими миллионы строк) может привести к существенным проблемам с производительностью. В Pandas данные хранятся в виде датафреймов или серий, и когда объем данных становится слишком большим для оперативной памяти, выполнение операций может замедлиться, что приводит к длительному времени выполнения и возможному падению системы из-за нехватки памяти. Вот основные проблемы, с которыми сталкиваются разработчики и аналитики:

1. **Высокие требования к памяти**: Большие датафреймы требуют значительных объемов оперативной памяти, особенно когда несколько копий данных создаются в процессе выполнения операций.
   
2. **Медленная скорость выполнения операций**: Некоторые операции в Pandas (например, циклические операции или методы, не оптимизированные для векторизации) могут значительно замедлить выполнение даже на современных машинах.

3. **Неэффективное использование ресурсов**: В случае, если код написан неэффективно, он может использовать больше ресурсов (включая CPU и память), чем необходимо для решения задачи.

### Причины низкой производительности

Основные причины, по которым операции в Pandas могут быть медленными или неэффективными, часто связаны с неправильным использованием API или архитектуры самой библиотеки. Рассмотрим основные из них.


#### 1. **Избыточные копии данных**

Одной из главных проблем, приводящих к ухудшению производительности, является **создание лишних копий данных**. Например:

- Когда вы выполняете операции, такие как фильтрация, добавление столбцов или изменения значений в датафрейме, по умолчанию Pandas создает копии данных. Это может существенно увеличить использование памяти и замедлить выполнение.
  
- **Пример:**

  ```python
  df = pd.read_csv('large_data.csv')  # Чтение данных в DataFrame
  df = df[df['column'] > 100]  # Фильтрация данных, создается копия
  ```

  В этом примере `df[df['column'] > 100]` создает новую копию датафрейма с отфильтрованными строками, что увеличивает потребление памяти, особенно для больших данных.

- **Как избежать**: Использование параметра `inplace=True` или явное управление памятью с помощью функций, которые выполняют операции без создания копий (например, `drop` с параметром `inplace=True`).

  ```python
  df.dropna(inplace=True)  # Удаление пропущенных значений без создания копии
  ```

  **Важно**: Важно понимать, что использование `inplace=True` может быть не всегда безопасным, так как изменения выполняются непосредственно в исходном объекте.

---

#### 2. **Неэффективные алгоритмы и операции**

Некоторые операции, особенно те, которые предполагают использование циклов Python или нестандартных методов, могут сильно замедлять работу. Pandas — это библиотека, ориентированная на векторизацию, что позволяет ускорить многие операции.

- **Циклы и `apply`**: Циклы `for` и использование метода `.apply()` часто оказываются гораздо медленнее, чем векторизированные операции. Особенно это заметно при работе с большими наборами данных.

  **Пример:**
  ```python
  # Использование apply (медленно)
  df['new_column'] = df['column'].apply(lambda x: x * 2)
  ```

  В данном случае операция над каждым элементом происходит поочередно, что является неэффективным.

  **Как ускорить**: Лучше использовать векторизированные операции. В Pandas существует множество встроенных методов, которые автоматически работают с целыми столбцами и выполняются гораздо быстрее.

  ```python
  # Векторизованный подход (быстро)
  df['new_column'] = df['column'] * 2
  ```

  Векторизация позволяет библиотеке использовать оптимизированный C-код, что значительно повышает производительность.

---

#### 3. **Частые объединения данных (merge, join)**

Операции объединения (например, `merge` и `join`) — это мощные инструменты, но они могут быть крайне ресурсоемкими, если объединяется несколько больших датафреймов. 

- **Проблема**: Неправильно выбранные индексы или частые объединения без предварительного индексирования могут замедлить процесс, так как Pandas вынужден выполнять полное сканирование всех строк.

  **Пример:**

  ```python
  df1 = pd.read_csv('data1.csv')
  df2 = pd.read_csv('data2.csv')
  df_merged = pd.merge(df1, df2, on='key_column')
  ```

  Без использования индексов объединение может привести к повторным поискам по всем строкам, что значительно увеличивает время выполнения.

- **Как улучшить**: Прежде чем выполнять операцию `merge`, создайте индекс по ключевому столбцу. Это значительно ускоряет выполнение операции.

  ```python
  df1.set_index('key_column', inplace=True)
  df2.set_index('key_column', inplace=True)
  df_merged = df1.join(df2)
  ```

  Здесь мы сначала устанавливаем индекс, а затем объединяем таблицы, что ускоряет процесс.

---

#### 4. **Использование неэффективных типов данных**

Тип данных, используемый для хранения информации в датафрейме, оказывает огромное влияние на производительность. Например, Pandas по умолчанию использует типы `float64`, `int64`, которые занимают много памяти, а если данные могут быть представлены меньшими типами, это приведет к излишнему расходу памяти.

- **Пример проблемы**: Когда датафрейм содержит столбец с небольшими целыми числами, но этот столбец имеет тип `int64`, это приводит к ненужным затратам памяти.

- **Решение**: Преобразование типов данных в более эффективные.

  **Пример:**
  ```python
  # Преобразуем столбцы в более компактные типы данных
  df['column'] = df['column'].astype('int32')
  ```

  Также стоит использовать категориальные типы для столбцов, содержащих повторяющиеся строки. Это значительно экономит память.

  ```python
  df['category_column'] = df['category_column'].astype('category')
  ```

  Это позволяет значительно снизить использование памяти, особенно если столбец содержит несколько уникальных значений.

## 2. Основные подходы к оптимизации

Оптимизация операций заключается в использовании эффективных методов работы с памятью, снижении избыточных вычислений и уменьшении затрат времени на выполнение операций. Разберем основные подходы:


### **1. Использование подходов "in-place"**

В Pandas многие операции, такие как изменение значений, фильтрация данных или удаление столбцов, могут выполняться "in-place", то есть без создания новых объектов данных. Это помогает сократить как потребление памяти, так и время, затраченное на выполнение операции.

#### **Что такое "in-place"?**
Когда вы выполняете операцию "in-place", изменения происходят непосредственно в исходном объекте без необходимости создания нового. Это важное преимущество, так как позволяет избежать дополнительных затрат на копирование данных в памяти.

- **Пример:**
  Если вы хотите удалить строки с пропущенными значениями, вы можете использовать метод `.dropna()`:

  ```python
  df.dropna()  # Создается новый объект, исходный df не изменяется
  ```

  Однако, если вы добавите параметр `inplace=True`, то изменения произойдут непосредственно в исходном объекте `df`:

  ```python
  df.dropna(inplace=True)  # Изменяет df "на месте"
  ```

#### **Зачем использовать "in-place"?**
- **Экономия памяти**: Избегаете создания лишних копий данных.
- **Ускорение работы**: Отсутствие необходимости в создании новых объектов снижает время выполнения операций.

##### **Когда не использовать "in-place"?**
Не всегда стоит полагаться на `inplace=True`. Иногда такие операции делают код менее читаемым и затрудняют отладку. Например, если вы хотите сохранить исходные данные для дальнейших операций, лучше не использовать "in-place" подход.

---

### **2. Снижение потребности в копиях данных**

Одной из основных проблем, которая влияет на производительность в Pandas, является **неправильное управление копиями данных**. Каждая операция, которая приводит к созданию нового датафрейма, увеличивает потребление памяти и замедляет выполнение.

##### **Причины создания копий**
Когда вы выполняете фильтрацию, изменение столбцов или операции с изменениями на уровне строк, Pandas может создать **копию данных**. Даже если вы неявно не просили об этом, Pandas может создать новый объект для того, чтобы не изменять исходный.

- **Пример:**
  ```python
  df = df[df['column'] > 10]  # Создается новая копия датафрейма
  ```

  Здесь создается новая копия датафрейма, что может быть неоправданным для больших данных.

#### **Как избежать избыточных копий?**

1. **Фильтрация и изменения "in-place"**:
   Использование операций с параметром `inplace=True` (например, для фильтрации, удаления строк и столбцов) или изменения значений непосредственно в столбцах без создания новых объектов.

   ```python
    df.drop(df[df['column'] <= value].index, inplace=True)
   ```

2. **Использование ссылки на объект**:
   Вместо того чтобы присваивать результат в новый объект, можно просто работать с оригинальным объектом напрямую, если это не нарушает логику работы с данными.

   ```python
   df = df[df['column'] < 100]  # Здесь df фактически копируется
   df.loc[df['column'] < 100, 'column'] = 0  # Более эффективное изменение
   ```

3. **Модификация данных без создания новых объектов**:
   Применение таких методов, как `replace`, `fillna`, `dropna` и т.д., с параметром `inplace=True`.

---

### **3. Векторизация операций и использование методов Pandas**

**Векторизация** — это один из важнейших инструментов для повышения производительности в Pandas. Векторизация позволяет выполнять операции над целыми столбцами или строками за один проход по данным, используя оптимизированный C-код, что значительно ускоряет работу по сравнению с использованием циклов.

##### **Что такое векторизация?**
Векторизация — это процесс преобразования алгоритмов, которые работают по одному элементу за раз (например, циклы), в такие, которые работают сразу с целыми массивами данных. В Pandas операции с целыми столбцами выполняются быстро, так как библиотека использует низкоуровневые операции (в основе Pandas лежат структуры NumPy, которые написаны на C и оптимизированы для выполнения операций с массивами).

#### **Пример векторизации**

- **Циклы (медленно):**
  Использование обычных циклов `for` в Python — это один из самых медленных способов обработки данных, так как каждая итерация цикла требует затрат времени на интерпретацию Python-кода.

  ```python
  df['new_column'] = 0
  for i in range(len(df)):
      df.loc[i, 'new_column'] = df.loc[i, 'column'] * 2
  ```

  Здесь мы вручную выполняем операцию для каждой строки, что крайне неэффективно.

- **Векторизация (быстро):**
  В Pandas, используя векторизованные операции, вы можете применить функцию сразу ко всем значениям столбца, что значительно ускоряет выполнение.

  ```python
  df['new_column'] = df['column'] * 2  # Векторизованная операция
  ```

  Векторизация позволяет Pandas использовать внутренние оптимизированные механизмы, такие как те, что в NumPy, где вся операция выполняется за один проход по данным.

#### **Использование встроенных методов Pandas**
Pandas предоставляет множество встроенных методов для работы с данными, которые уже реализованы с использованием векторизации:

- **Пример использования встроенных функций**:

  Вместо того чтобы применить функцию через `apply()` (которая может быть медленной), используйте встроенные методы Pandas:

  ```python
  df['new_column'] = df['column'].apply(lambda x: x * 2)  # Медленный подход
  ```

  Лучше использовать:

  ```python
  df['new_column'] = df['column'] * 2  # Векторизованный подход
  ```

  В последнем примере операция умножения будет выполняться на уровне C, что ускоряет выполнение задачи.

## 3. Использование типов данных для экономии памяти

Один из ключевых аспектов работы с большими данными в Pandas — это эффективное использование памяти. **Оптимизация типов данных** может существенно снизить потребление памяти и улучшить производительность. 

### **Почему оптимизация типов данных важна?**

Когда мы работаем с большими датафреймами, даже незначительное изменение типа данных может привести к значительной экономии памяти. Например:

- Столбцы с числами, представленными типом `float64`, могут занять в два раза больше памяти, чем те же числа, но с типом `float32`.
- Строки в Pandas занимают много памяти, а использование **категориальных типов** позволяет сократить память за счет хранения уникальных значений только один раз.

Оптимизация типов данных — это не только экономия памяти, но и улучшение производительности операций, таких как агрегации или сортировки.

---

### **1. Перевод данных в оптимальные типы**

В Pandas можно явно изменять типы данных столбцов с помощью метода `.astype()`. Это полезно, если вы хотите сократить использование памяти или улучшить производительность. Например, если вы работаете с целыми числами, которые точно помещаются в более узкий тип данных, это может существенно уменьшить память, которую использует ваш датафрейм.

#### **Примеры:**

- **Числовые данные**: если у вас есть столбец с целыми числами, но значения в нем не выходят за пределы диапазона, подходящего для меньшего типа (например, `int8`), вы можете преобразовать столбец в этот тип.
  
  - `int64` занимает 8 байт на число, в то время как `int8` — только 1 байт. Преобразование `int64` в `int8` может сэкономить память, если значения находятся в диапазоне от -128 до 127.

  - **Пример:**

    ```python
    df['age'] = df['age'].astype('int8')  # Если возраст лежит в пределах от -128 до 127
    ```

  - Этот подход помогает минимизировать потребление памяти, особенно если вы работаете с большими наборами данных.

- **Числовые данные с плавающей запятой**: по умолчанию Pandas использует `float64` для столбцов с вещественными числами, однако для многих задач достаточно `float32`, что позволяет сэкономить половину памяти.

  - **Пример:**

    ```python
    df['price'] = df['price'].astype('float32')  # Если значения вещественные, но не требуют высокой точности
    ```

    Важно помнить, что с `float32` точность чисел может быть ограничена, поэтому его лучше использовать только в тех случаях, когда высокая точность не требуется.

---

### **2. Использование категориальных типов для строк**

Строки в Pandas — это объекты типа `object`, которые могут занимать значительную память, особенно если они содержат много повторяющихся значений. Вместо того, чтобы хранить каждое значение строки в памяти отдельно, можно преобразовать столбец строк в **категориальный тип**. Это полезно, если столбец содержит ограниченное количество уникальных строк (например, категории, метки, коды и т. д.).

#### **Что такое категориальный тип данных?**
Категориальные типы в Pandas — это специальный тип данных, который эффективно кодирует строковые данные в виде числовых меток, используя **словарь уникальных значений**. Каждый уникальный элемент в столбце будет представлен числовым значением (индексом), что экономит память. Например, столбец с 1000 строками, содержащими всего 10 уникальных значений, будет занимать значительно меньше памяти, если использовать категориальный тип.

- **Преимущества**:
  - Экономия памяти.
  - Ускорение операций с такими столбцами (например, сортировки, группировки).

#### **Пример: преобразование строк в категории**

Предположим, что у вас есть столбец с названиями городов, но на самом деле в нем всего 10 уникальных значений, и большинство строк повторяются. Мы можем преобразовать этот столбец в категориальный тип:

```python
df['city'] = df['city'].astype('category')
```

После этого Pandas будет хранить только 10 уникальных значений (вместо хранения каждой строки отдельно), что может сократить потребление памяти в несколько раз.

- **Сколько памяти можно сэкономить?**
  - Строки могут занимать от 50 до 100 байт в зависимости от их длины.
  - Категориальные типы данных могут занимать всего несколько байт на уникальное значение (например, 1 или 2 байта для числовых меток).

#### **Когда использовать категориальные типы?**

Категориальные типы идеально подходят для столбцов, содержащих:
- Категории (например, типы продуктов, состояния, регионы).
- Метки, коды, флаги, которые часто повторяются.
- Строки с ограниченным набором уникальных значений.

---

### **3. Оптимизация типов данных для числовых столбцов**

Числовые столбцы — это одно из наиболее часто встречающихся типов данных в Pandas. Однако Pandas по умолчанию использует типы данных, такие как `int64` или `float64`, которые занимают много памяти, даже если данные в столбцах могут быть представлены меньшими типами.

#### **Примеры оптимизации типов данных для числовых столбцов**:

- **Целые числа**:
  Если столбец содержит только небольшие целые числа, их можно привести к более компактным типам данных, например, `int8`, `int16` или `int32`.

  - **Пример**:

    ```python
    df['small_numbers'] = df['small_numbers'].astype('int8')  # Для значений от -128 до 127
    ```

  Важно помнить, что использование слишком маленького типа (например, `int8` для чисел, превышающих 127) приведет к потере данных или переполнению.

- **Числа с плавающей запятой**:
  Если ваши числа не требуют высокой точности, можно использовать `float32` вместо `float64`.

  - **Пример**:

    ```python
    df['float_column'] = df['float_column'].astype('float32')  # Если высокая точность не требуется
    ```

  Это может значительно сэкономить память, так как `float32` использует только 4 байта на число, в то время как `float64` использует 8 байт.

#### **Другие оптимизации:**
- **Использование `pd.to_datetime()` для дат**: Если ваши данные содержат столбцы с датами, конвертируйте их в тип `datetime64` с помощью метода `pd.to_datetime()`, чтобы они занимали меньше памяти и оптимизировали вычисления.

  - **Пример:**

    ```python
    df['date'] = pd.to_datetime(df['date'])
    ```

- **Использование `timedelta64` для разностей дат**: Если вам нужно хранить разницу между датами, лучше использовать тип `timedelta64`, чтобы сэкономить память.



## 4. Индексация данных для ускорения операций

Индексация — это ключевой механизм, который значительно ускоряет доступ к данным в Pandas. В этом разделе мы рассмотрим, как эффективное использование индексов может улучшить производительность операций, таких как фильтрация, объединение и группировка. Мы также обсудим, как использование **многоколоночных** и **многоуровневых индексов** позволяет организовать данные таким образом, чтобы работать с ними было быстрее и удобнее.

---

### **1. Оптимизация доступа к данным с помощью многоколоночных и многоуровневых индексов**

**Индексы в Pandas** — это метки или структуры, которые позволяют быстро находить данные в таблице, не проводя полное сканирование. Использование правильных индексов помогает ускорить доступ к данным, особенно при работе с большими объемами информации. Многоколоночные и многоуровневые индексы играют важную роль в оптимизации производительности.

#### **Многоколоночный индекс**

Многоколоночный индекс, или **многоколоночное индексирование**, позволяет использовать более одного столбца в качестве индекса, что позволяет ускорить доступ к данным при сложных запросах. В отличие от стандартного индекса, который используется для уникальной идентификации строк по одному столбцу, многоколоночный индекс позволяет строить иерархию данных.

Например, в DataFrame можно использовать два столбца как индексы, чтобы ускорить операции, такие как фильтрация или поиск:

```python
import pandas as pd

# Создадим DataFrame с двумя столбцами индекса
df = pd.DataFrame({
    'region': ['North', 'South', 'East', 'West', 'North', 'South'],
    'category': ['A', 'B', 'A', 'B', 'B', 'A'],
    'sales': [100, 200, 150, 250, 120, 180]
})

# Установим многоколоночный индекс
df.set_index(['region', 'category'], inplace=True)

print(df)
```

**Вывод:**
```
               sales
region category      
North A          100
South B          200
East A           150
West B           250
North B          120
South A          180
```

Здесь индекс состоит из двух столбцов: `region` и `category`. Это позволяет выполнять запросы по комбинации этих двух значений, значительно ускоряя операции поиска.

#### **Преимущества многоколоночного индекса:**

1. **Быстрая фильтрация**: позволяет быстро фильтровать данные по комбинации значений нескольких столбцов.
   
   Например, чтобы найти продажи для региона "North" и категории "A", можно просто обратиться к индексу:

   ```python
   df.loc[('North', 'A')]
   ```

2. **Упрощение операций группировки**: позволяет легко выполнять группировки, агрегируя данные по нескольким уровням.

3. **Использование более сложных запросов**: вы можете фильтровать и работать с данными на основе комбинации различных столбцов индекса.

#### **Многоуровневый индекс**

**Многоуровневые индексы** (или **MultiIndex**) — это расширение идеи многоколоночного индекса. MultiIndex позволяет создавать индексы, состоящие из нескольких уровней. Каждый уровень может быть отдельным столбцом, и вы можете индексировать данные по этим уровням.

Пример использования многоуровневого индекса:

```python
# Создадим DataFrame с многоуровневым индексом
index = pd.MultiIndex.from_tuples([('North', 'A'), ('South', 'B'), ('East', 'A'), 
                                   ('West', 'B'), ('North', 'B'), ('South', 'A')],
                                  names=['region', 'category'])

df = pd.DataFrame({
    'sales': [100, 200, 150, 250, 120, 180]
}, index=index)

print(df)
```

**Вывод:**
```
               sales
region category      
North A          100
South B          200
East A           150
West B           250
North B          120
South A          180
```

В этом примере у нас два уровня индекса: `region` и `category`. Такой подход позволяет более гибко и быстро работать с данными, выполняя сложные операции по одному или нескольким уровням индекса.

#### **Преимущества многоуровневого индекса:**

1. **Гибкость при фильтрации**: можно фильтровать данные по одному или нескольким уровням индекса. Например, для получения всех строк для региона `North`:

   ```python
   df.xs('North', level='region')
   ```

2. **Упрощение агрегаций и сводок**: позволяет удобно агрегировать данные по разным уровням индекса, например, по региону и категории:

   ```python
   df.groupby(level='region').sum()  # Группировка по региону
   ```

3. **Быстрота поиска**: при многоколоночном или многоуровневом индексе операции поиска становятся значительно быстрее, чем при использовании обычного индекса.

---

### **2. Использование индексов для улучшения производительности при объединении таблиц**

Одной из наиболее важных оптимизаций с использованием индексов является улучшение производительности при объединении данных (например, с помощью операций `merge`, `join` или `concat`). Индексы позволяют значительно ускорить процесс объединения таблиц, поскольку они уже организуют данные в порядке, который упрощает их слияние.

#### **Пример объединения с индексами**

Предположим, у нас есть два DataFrame, и мы хотим их объединить. Один из способов ускорить эту операцию — использовать индексы для объединения.

1. Создадим два DataFrame с индексами:

```python
# Создаем первый DataFrame с многоколоночным индексом
df1 = pd.DataFrame({
    'region': ['North', 'South', 'East', 'West'],
    'category': ['A', 'B', 'A', 'B'],
    'sales': [100, 200, 150, 250]
})

df1.set_index(['region', 'category'], inplace=True)

# Создаем второй DataFrame с аналогичной структурой
df2 = pd.DataFrame({
    'region': ['North', 'South', 'East', 'West'],
    'category': ['A', 'B', 'A', 'B'],
    'profit': [30, 60, 40, 80]
})

df2.set_index(['region', 'category'], inplace=True)

# Выполним объединение
merged_df = df1.join(df2)

print(merged_df)
```

**Вывод:**
```
               sales  profit
region category              
North A          100      30
South B          200      60
East A           150      40
West B           250      80
```

Здесь оба DataFrame имеют многоколоночные индексы с уровнями `region` и `category`. Когда мы используем метод `join()`, индексы автоматически используются для быстрого объединения данных. В результате объединение происходит гораздо быстрее, чем если бы мы объединяли таблицы по обычным столбцам.

#### **Оптимизация объединений с использованием индексов**:
- **Использование индексов для слияний**: Когда вы объединяете DataFrame по индексам, это обычно быстрее, чем объединение по столбцам, потому что индексы уже упорядочены.
- **Оптимизация слияний по несколько столбцам**: Использование многоколоночных индексов позволяет избежать необходимости использования дополнительных параметров в методах `merge` или `join`.

#### **Когда использовать индексы для объединений?**
- Когда у вас есть большие таблицы, и объединение их по столбцам может быть медленным.
- Когда данные уже упорядочены и имеют схожие структуры индексов.
- Для многоколоночных и многоуровневых объединений, которые требуют фильтрации или группировки по нескольким столбцам.

## 5. Векторизация и использование lambda-функций

Когда мы работаем с большими данными, важно помнить, что **эффективность операций** играет ключевую роль. **Векторизация** и использование **lambda-функций** — это два мощных способа ускорить выполнение операций в **Pandas**. Эти подходы позволяют снизить накладные расходы, связанные с циклическими операциями и оптимизировать работу с данными, значительно повышая производительность кода.

---

### **1. Использование Pandas векторизованных функций для ускорения операций**

Векторизация — это процесс выполнения операций над массивами данных без явных циклов Python. Вместо того чтобы обходить строки или столбцы по одному элементу, Pandas (и NumPy) позволяют выполнять операции над целыми массивами данных одновременно, используя оптимизированный C-код. Это значительно ускоряет выполнение операций, так как сам процесс обработки происходит на низком уровне (в C), что быстрее, чем обработка в Python.

#### **Пример векторизации с Pandas**

Предположим, что у нас есть DataFrame, и мы хотим применить к нему простую арифметическую операцию — например, умножить все значения в одном столбце на 2.

```python
import pandas as pd

# Создаем DataFrame
df = pd.DataFrame({
    'A': [1, 2, 3, 4, 5],
    'B': [10, 20, 30, 40, 50]
})

# Векторизация: умножаем столбец 'A' на 2
df['A'] = df['A'] * 2

print(df)
```

**Вывод:**
```
   A   B
0  2  10
1  4  20
2  6  30
3  8  40
4  10 50
```

В этом примере операция умножения на 2 была выполнена **векторизованно**, то есть над всем столбцом одновременно, без явных циклов. Это значительно быстрее, чем если бы мы использовали цикл, например, через `apply`.

#### **Преимущества векторизации**:

1. **Производительность**: Векторизация выполняется быстрее, чем использование циклов, так как Pandas и NumPy используют оптимизированные низкоуровневые операции.
2. **Читаемость кода**: Векторизация позволяет писать более краткий и понятный код.
3. **Простота**: Простые операции, такие как арифметика или логические сравнения, легко выполняются без использования циклов.

#### **Пример: Применение математической функции к столбцу**

Pandas предоставляет набор векторизованных функций, которые могут быть применены к DataFrame или Series. Например, можно быстро вычислить логарифм значений в столбце:

```python
import numpy as np

# Применяем векторизованную функцию np.log
df['log_B'] = np.log(df['B'])

print(df)
```

**Вывод:**
```
   A   B    log_B
0  2  10  2.302585
1  4  20  2.995732
2  6  30  3.401197
3  8  40  3.688879
4  10 50  3.912023
```

Здесь функция `np.log()` применяется ко всем элементам столбца `B` одновременно, что делает код эффективным и быстрым.

---

### **2. Использование `apply` и `map` для оптимизации работы с функциями**

В случае, когда стандартная векторизация не подходит или когда нужно применить более сложную логику, можно использовать функции `apply` и `map`. Эти методы предоставляют гибкость для применения произвольных функций к данным, но их производительность значительно ниже, чем у векторизованных операций. Тем не менее, их использование может быть полезным для определенных типов задач.

#### **Метод `apply`**

Метод `apply` позволяет применять функцию к каждому элементу в столбце или строке. В отличие от векторизованных операций, `apply` выполняет функцию поэтапно для каждой строки или столбца.

**Пример использования `apply` для обработки строк:**

```python
# Применяем lambda-функцию для преобразования строк
df['A_squared'] = df['A'].apply(lambda x: x ** 2)

print(df)
```

**Вывод:**
```
   A   B  A_squared
0  2  10          4
1  4  20         16
2  6  30         36
3  8  40         64
4  10 50        100
```

Здесь функция `lambda x: x ** 2` применяется к каждому значению в столбце `A` для возведения в квадрат. Несмотря на то что метод `apply` не такой быстрый, как векторизация, он все же позволяет гибко манипулировать данными и применить более сложные функции.

#### **Когда использовать `apply`?**
- Когда вам нужно применить сложную функцию, которая не может быть векторизована.
- Когда вы хотите обработать данные по строкам или столбцам.
- Когда функция не может быть представлена с помощью стандартных векторизованных операций.

#### **Метод `map`**

Метод `map` используется для применения функции к каждому элементу в одном столбце. Это облегчает преобразования значений, особенно когда столбец содержит категориальные данные или данные, требующие замены значений.

**Пример использования `map` для замены значений:**

```python
# Сопоставление значений с помощью map
df['Category'] = df['A'].map({2: 'Low', 4: 'Medium', 6: 'High', 8: 'Very High', 10: 'Extreme'})

print(df)
```

**Вывод:**
```
   A   B  A_squared   Category
0  2  10          4        Low
1  4  20         16     Medium
2  6  30         36       High
3  8  40         64  Very High
4  10 50        100     Extreme
```

Здесь метод `map` помогает заменить числовые значения в столбце `A` на категориальные метки в столбце `Category`. Это удобный способ для применения логики замены значений в одном столбце.

#### **Когда использовать `map`?**
- Когда нужно заменить значения в столбце по заранее определенному словарю.
- Когда вы работаете с категориальными данными и хотите провести маппинг или преобразование значений.

---

### **Сравнение векторизации, `apply` и `map`**

- **Векторизация**: Это самый быстрый и эффективный способ обработки данных. Он оптимизирован на уровне C и позволяет работать с большими объемами данных без циклов.
  - **Использование**: предпочтительный способ для математических операций, логических операций и стандартных преобразований.
  - **Производительность**: Высокая.

- **`apply`**: Это более гибкий способ применения произвольных функций к данным. Он работает медленнее, чем векторизация, но подходит для более сложных операций, которые нельзя выполнить с помощью векторизации.
  - **Использование**: применяется, когда нужно использовать сложные функции, которые не могут быть векторизованы.
  - **Производительность**: Низкая по сравнению с векторизацией.

- **`map`**: Это удобный способ для замены значений в одном столбце. Он подходит для работы с категорическими данными и позволяет заменить или преобразовать значения.
  - **Использование**: предпочтителен для замены значений по словарю.
  - **Производительность**: Выше, чем у `apply`, но ниже, чем у векторизации.

## 6. Применение многозадачности и параллельных вычислений

В работе с большими данными, особенно когда объем данных значительно превышает объем оперативной памяти, обычные инструменты, такие как Pandas, могут столкнуться с проблемами производительности. В таких случаях возникает необходимость в **многозадачности** и **параллельных вычислениях**. Одним из наиболее мощных инструментов для таких целей является библиотека **Dask**, которая предоставляет параллельные вычисления, масштабируемые на многозадачные системы и распределенные вычисления.

---

### **1. Использование Dask для параллельных вычислений с большими данными**

**Dask** — это библиотека для параллельных вычислений и распределенной обработки данных. Она предоставляет удобные интерфейсы, аналогичные Pandas, но в отличие от Pandas, Dask работает с большими данными, не загружая их целиком в память. Это достигается с помощью использования **Dask DataFrame**, который представляет собой **множество Pandas DataFrame**, распределенных по нескольким ядрам или даже нескольким машинам.

#### **Как работает Dask?**

- **Распределенные вычисления**: Dask разбивает задачу на небольшие подзадачи, которые могут выполняться параллельно на нескольких ядрах или даже на нескольких машинах.
- **Отложенные вычисления (Lazy Evaluation)**: Dask использует отложенные вычисления, что означает, что операции на данных не выполняются немедленно, а формируется граф вычислений. Это позволяет Dask оптимизировать выполнение задач.
- **Масштабируемость**: Dask способен работать с данными, которые не помещаются в оперативную память, распределяя их по нескольким процессам или машинам.

#### **Почему использовать Dask?**
- **Больше данных**: Dask помогает работать с большими данными, которые не помещаются в память.
- **Параллельные вычисления**: Он автоматически распределяет вычисления по всем доступным ядрам процессора или даже по всему кластеру.
- **Совместимость с Pandas**: API Dask напоминает Pandas, что облегчает переход для тех, кто уже знаком с Pandas.

---

### **2. Пример: распараллеливание операций с Dask DataFrame**

Чтобы понять, как работает Dask, давайте рассмотрим пример с использованием **Dask DataFrame**. Мы будем работать с большими данными и распараллелим операции, такие как фильтрация и агрегация.

#### **Пример создания Dask DataFrame и распараллеливания операций**

1. **Установка Dask**:

   Если у вас еще не установлен Dask, установите его с помощью pip:

   ```bash
   pip install dask[complete]
   ```

2. **Создание Dask DataFrame**:

   Сначала создадим пример данных в Pandas, а затем преобразуем их в Dask DataFrame для параллельной обработки.

   ```python
   import dask.dataframe as dd
   import pandas as pd
   import numpy as np

   # Создание большого DataFrame в Pandas
   df_pandas = pd.DataFrame({
       'A': np.random.rand(10**7),
       'B': np.random.rand(10**7)
   })

   # Преобразование Pandas DataFrame в Dask DataFrame
   ddf = dd.from_pandas(df_pandas, npartitions=4)  # Разбиваем на 4 части
   ```

   **Что происходит?**
   - Мы создали **Pandas DataFrame** с 10 миллионами строк.
   - Затем мы преобразовали его в **Dask DataFrame** с 4 частями (партиями), чтобы Dask мог параллельно обрабатывать эти данные.

3. **Параллельная агрегация с использованием Dask**:

   Давайте теперь выполним операцию агрегации (среднее значение) по столбцу `A`:

   ```python
   # Параллельное вычисление среднего значения столбца 'A'
   result = ddf['A'].mean().compute()  # .compute() запускает вычисления
   print(result)
   ```

   **Что происходит?**
   - Операция `mean()` выполняется параллельно по всем разделам данных.
   - Метод `compute()` выполняет вычисления и собирает результаты. До этого момента Dask не выполнял вычисления, так как он использует отложенную вычислительную модель.

   **Преимущества:**
   - Если данных слишком много для одного устройства, Dask автоматически распределяет вычисления по нескольким ядрам.
   - Мы используем только часть данных, а остальные находятся в другом процессе или машине, что позволяет не превышать объем оперативной памяти.

---

### **3. Когда применять параллельные вычисления, а когда это не имеет смысла**

Применение параллельных вычислений, как правило, дает хорошие результаты, когда **объем данных велик**, и **операции сложны**. Однако не всегда стоит использовать Dask или другие параллельные вычисления. Важно понимать, когда это оправдано, а когда может привести к излишним затратам ресурсов.

#### **Когда имеет смысл использовать Dask:**

1. **Объем данных превышает память**:
   Если данные слишком большие для одного устройства (например, превышают объем оперативной памяти), Dask может эффективно распределять их между несколькими машинами или процессами.
   
2. **Вычисления требуют значительных затрат времени**:
   Когда операции, такие как сложные агрегации, группировки или фильтрации, занимают слишком много времени, Dask позволяет распараллелить их, ускорив выполнение.

3. **Многоядерные и распределенные вычисления**:
   Если ваша задача может быть разбита на независимые подзадачи (например, обработка больших данных или обработка данных в реальном времени), Dask предоставляет удобные средства для распараллеливания и распределения вычислений.

4. **Масштабируемость**:
   Dask позволяет работать не только на одном компьютере, но и на большом распределенном кластере, что удобно для очень крупных наборов данных (например, нескольких терабайт).

#### **Когда не имеет смысла использовать Dask:**

1. **Малые данные**:
   Если данные помещаются в память, и их размер не превышает нескольких гигабайт, использование Dask не принесет пользы, а наоборот, может даже замедлить выполнение из-за накладных расходов на управление параллельными вычислениями. В этом случае Pandas будет работать быстрее, так как не будет затрат на распределение данных.

2. **Низкий параллелизм**:
   Если ваша задача выполняет слишком простые или быстрые операции, распределение вычислений по ядрам может быть неэффективным. Например, если задача выполняется быстро и требует минимальных вычислительных ресурсов, накладные расходы на управление многозадачностью могут перевесить выгоды.

3. **Малые и средние задачи с последовательной обработкой**:
   Для небольших наборов данных, которые могут быть обработаны последовательно, параллельные вычисления могут добавить лишнюю сложность и замедлить выполнение. Например, для загрузки и обработки данных, которые помещаются в память, нет необходимости в параллельной обработке.
